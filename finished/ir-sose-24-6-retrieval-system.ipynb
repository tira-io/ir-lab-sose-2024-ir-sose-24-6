{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Imports f端r alle Notebooks\n",
    "\n",
    "!pip3 install tira ir-datasets python-terrier nltk scikit-learn spacy\n",
    "\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import os\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der NLTK Ressourcen\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Laden der SpaCy-Ressourcen\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Laden des SpaCy-Modells\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode um Beschreibung des POS-Tags zu bekommen f端r den NLTK Lemmatizer\n",
    "def get_wordnet_pos_nltk(treebank_tag):\n",
    "    \"\"\"Konvertiert POS-Tag in ein Format, das vom WordNet-Lemmatizer unterst端tzt wird.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Funktion um den Text zu lemmatizen f端r NLTK Lemmatizer\n",
    "def lemmatize_text_nltk(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos_nltk(tag)) for token, tag in pos_tags]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Definition der Funktion zur Lemmatization eines Textes mit SpaCy\n",
    "def lemmatize_text_spacy(text):\n",
    "    \"\"\"Lemmatiziert den gegebenen Text mit SpaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def preprocess_documents(documents, method):\n",
    "    if method == 'nltk':\n",
    "        lemmatize_text = lemmatize_text_nltk\n",
    "    elif method == 'spacy':\n",
    "        lemmatize_text = lemmatize_text_spacy\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method specified. Use 'nltk' or 'spacy'.\")\n",
    "\n",
    "    for doc in documents:\n",
    "        doc['text'] = lemmatize_text(doc['text'])\n",
    "        yield doc\n",
    "\n",
    "#Funktion um eigene Indecies zu erstellen\n",
    "def create_index(base_path, documents, stopwords, stemmer):\n",
    "    # Generate a unique identifier based on current timestamp\n",
    "    unique_id = hashlib.sha1(str(time.time()).encode('utf-8')).hexdigest()[:8]\n",
    "    \n",
    "    # Construct the unique path using base_path and unique_id\n",
    "    index_path = os.path.join(base_path, f\"index_{unique_id}/\")\n",
    "    \n",
    "    indexer = pt.IterDictIndexer(index_path, overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords, stemmer=stemmer)\n",
    "    index_ref = indexer.index(documents)\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Funktion um aus einem txt-file eine Python Liste zu machen\n",
    "def read_text_file_to_array(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            # Remove newline characters and convert to integers\n",
    "            array = [(line.strip()) for line in lines]\n",
    "            return array\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load custom stopword lists\n",
    "terrier_custom_stopwords = read_text_file_to_array('../terrier-custom.txt')\n",
    "chatgpt_stopwords = read_text_file_to_array('../chatgpt-stopwordlist.txt')\n",
    "\n",
    "base_path = '/workspaces/ir-lab-sose-2024-ir-sose-24-6/gridsearch/var/tmp/'\n",
    "\n",
    "S_E_T_index = create_index(base_path, pt_dataset.get_corpus_iter(), terrier_custom_stopwords, 'EnglishSnowballStemmer')\n",
    "\n",
    "S_E_T_XSqrA_M = pt.BatchRetrieve(S_E_T_index, wmodel=\"XSqrA_M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = S_E_T_XSqrA_M(pt_dataset.get_topics('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_and_normalize_run(run, system_name='bm25-baseline', default_output='../runs')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
