{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Jupyter Notebook beschäftigen wir uns mit der Lemmatisierung einer Dokumentensammlung. Dieser Schritt gehört zum Preprocessing der Dokumente. Die Grundidee der Lemmatisierung ist das Reduzieren von Wörtern auf ihre Stammform, auch Lemma genannt.\n",
    "\n",
    "Lemmatisierung und Stemming haben gemeinsam, dass sie vor allem Suffixe (Nachsilben) und teilweise auch Präfixe entfernen. Der wesentliche Unterschied zwischen den beiden Verfahren liegt jedoch darin, dass Stemming-Algorithmen ohne Kontext arbeiten. Das bedeutet, dass die resultierenden Wortformen teilweise keinen Sinn ergeben oder falsche Grundformen darstellen. Lemmatisierungsalgorithmen hingegen berücksichtigen den Kontext jedes Wortes im Satz und schlagen es in einem Wörterbuch nach, um die korrekte Grundform zu finden. Dadurch entstehen keine falschen Grundformen.\n",
    "\n",
    "Beispiel:\n",
    "\n",
    "Original: \"The foxes are running swiftly.\"\n",
    "    Stemming: \"The fox are run swift.\"\n",
    "    Lemmatization: \"The fox be run swiftly.\"\n",
    "\n",
    "Original: \"She enjoys reading books.\"\n",
    "    Stemming: \"She enjoy read book.\"\n",
    "    Lemmatization: \"She enjoy reading book.\"\n",
    "\n",
    "Wir beschäftigen uns mit den zwei geläufigsten Lemmatizern: dem WordNet Lemmatizer aus der NLTK Library und dem SpaCy Lemmatizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Imports für alle Notebooks\n",
    "\n",
    "!pip3 install tira ir-datasets python-terrier nltk spacy\n",
    "\n",
    "\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "\n",
    "# Loading the NLTK-Ressources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Laden der SpaCy-Ressourcen\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Laden des SpaCy-Modells\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')\n",
    "\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordNet Lemmatizer\n",
    "\n",
    "Der WordNet Lemmatizer basiert auf der lexikalischen Datenbank WordNet. Diese besitzt Informationen über die semantischen Relationen zwischen Wörtern. Diese sind in Gruppen bedeutungsähnlicher Wörter eingeteilt und haben z.B. Informationen über Synonyme, Antonyme oder Hyperonyme. Diese Gruppen werden auch Synsets genannt.\n",
    "\n",
    "Vorgehensweise\n",
    "\n",
    "1.) Wörterbuchabgleich**:\n",
    "    Der Lemmatizer prüft ein Wort gegen WordNet, um das Lemma zu finden.\n",
    "\n",
    "2.) POS-Tag (Part of Speech)**:\n",
    "    Mit Hilfe des POS-Tags können wir die Wortart eines Wortes im Kontext des Satzes identifizieren und klassifizieren. Wortarten sind z.B. Nomen, Verben, Adverben oder Adjektive. Damit können wir dann die genaue Grundform des Wortes bestimmen.\n",
    "\n",
    "    Beispiel:\n",
    "    - Verb: running -> run\n",
    "    - Nomen: running -> running\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the describtion of the POS Tags\n",
    "def get_wordnet_pos_nltk(treebank_tag):\n",
    "    \"\"\"Konvertiert POS-Tag in ein Format, das vom WordNet-Lemmatizer unterstützt wird.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize a text\n",
    "def lemmatize_text_nltk(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos_nltk(tag)) for token, tag in pos_tags]\n",
    "    return ' '.join(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Funktion zur Vorverarbeitung von Dokumenten\n",
    "def preprocess_documents_nltk(documents):\n",
    "    \"\"\"Anwendet Lemmatization auf alle Dokumente.\"\"\"\n",
    "    for doc in documents:\n",
    "        doc['text'] = lemmatize_text_nltk(doc['text'])\n",
    "        yield doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Funktion zum Erstellen eines Index\n",
    "def create_index_nltk(documents, stopwords):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords)\n",
    "    index_ref = indexer.index(preprocess_documents_nltk(documents))\n",
    "    return pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Funktion zum Lesen einer Textdatei und Konvertieren in ein Array\n",
    "def read_text_file_to_array(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            # Entfernen von Zeilenumbrüchen und Konvertierung in Strings\n",
    "            array = [line.strip() for line in lines]\n",
    "            return array\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy Lemmatizer\n",
    "\n",
    "Der SpaCy Lemmatizer basiert vor allem auf vortrainierten Modellen.\n",
    "\n",
    "Vorgehensweise\n",
    "\n",
    "1.) POS (Part of Speech):\n",
    "        SpaCy identifiziert und klassifiziert Wortarten mit Hilfe von vortrainierten Algorithmen und Modellen.\n",
    "\n",
    "2.) Lemmatization:\n",
    "        SpaCy verwendet Regeln und Wortlisten, um das Lemma für ein Wort zu finden. Diese Regeln und Wortlisten sind in den Sprachmodellen von SpaCy eingebettet und basieren auf umfangreichen Trainingsdaten.\n",
    "\n",
    "3.) Kontext:\n",
    "        SpaCy berücksichtigt den ganzen Satz und nicht nur das isolierte Wort, um die Lemmas zu finden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Funktion zur Lemmatization eines Textes mit SpaCy\n",
    "def lemmatize_text_spacy(text):\n",
    "    \"\"\"Lemmatiziert den gegebenen Text mit SpaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Definition der Funktion zur Vorverarbeitung von Dokumenten\n",
    "def preprocess_documents_spacy(documents):\n",
    "    \"\"\"Anwendet Lemmatization auf alle Dokumente.\"\"\"\n",
    "    for doc in documents:\n",
    "        doc['text'] = lemmatize_text_spacy(doc['text'])\n",
    "        yield doc\n",
    "\n",
    "# Definition der Funktion zum Erstellen eines Index\n",
    "def create_index_spacy(documents, stopwords):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords)\n",
    "    index_ref = indexer.index(preprocess_documents_spacy(documents))\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lesen der Stopwords-Datei und Konvertieren in ein Array\n",
    "file_path = \"../terrier-stopwordlist.txt\"\n",
    "stopwords = read_text_file_to_array(file_path)\n",
    "\n",
    "\n",
    "costum_index_nltk =create_index_nltk(pt_dataset.get_corpus_iter(), stopwords)\n",
    "# Erstellen des benutzerdefinierten Index mit Lemmatization\n",
    "costum_index_spacy = create_index_spacy(pt_dataset.get_corpus_iter(), stopwords)\n",
    "\n",
    "# Erstellen der BatchRetrieve-Instanzen\n",
    "XSqrA_M = pt.BatchRetrieve(index, wmodel=\"XSqrA_M\")\n",
    "XSqrA_M_nltk = pt.BatchRetrieve(costum_index_nltk, wmodel=\"XSqrA_M\")\n",
    "XSqrA_M_spacy = pt.BatchRetrieve(costum_index_spacy, wmodel=\"XSqrA_M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation der Modelle\n",
    "pt.Experiment(\n",
    "    [XSqrA_M, XSqrA_M_nltk, XSqrA_M_spacy],\n",
    "    pt_dataset.get_topics(),\n",
    "    pt_dataset.get_qrels(),\n",
    "    [\"ndcg_cut.10\", \"recip_rank\", \"recall_100\", \"P_10\"],\n",
    "    names=[\"XSqrA_M\", \"XSqrA_M_NLTK\", \"XSqrA_M_SPACY\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
