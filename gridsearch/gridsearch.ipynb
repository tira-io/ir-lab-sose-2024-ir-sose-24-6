{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das ist das fertige Notebook, welches aus den Test zu Stopwords hervorgegangen ist. \n",
    "Es sind jetzt erstmal fünf indecies drinnen, Standard (Terrier), Terrier_costum, Terrier_query_occur, ChatGPT und Nostopwords.\n",
    "Wobei Terrier_costum und Terrier_query_occur natürlich bisschen Overfitting sind, aber es sind auch die besten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.132)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tira) (6.1.3)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (23.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (1.7.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (1.26.2)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Collecting click\n",
      "  Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 KB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2024.5.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (775 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m775.1/775.1 KB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "Installing collected packages: regex, click, nltk\n",
      "Successfully installed click-8.1.7 nltk-3.8.1 regex-2024.5.15\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTerrier 0.10.0 has loaded Terrier 5.8 (built by craigm on 2023-11-01 18:05) and terrier-helper 0.0.8\n",
      "\n",
      "No etc/terrier.properties, using terrier.default.properties for bootstrap configuration.\n"
     ]
    }
   ],
   "source": [
    "#Standard Imports für alle Notebooks\n",
    "\n",
    "!pip3 install tira ir-datasets python-terrier nltk scikit-learn spacy\n",
    "\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der NLTK Ressourcen\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Laden der SpaCy-Ressourcen\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Laden des SpaCy-Modells\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode um Beschreibung des POS-Tags zu bekommen für den NLTK Lemmatizer\n",
    "def get_wordnet_pos_nltk(treebank_tag):\n",
    "    \"\"\"Konvertiert POS-Tag in ein Format, das vom WordNet-Lemmatizer unterstützt wird.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Funktion um den Text zu lemmatizen für NLTK Lemmatizer\n",
    "def lemmatize_text_nltk(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos_nltk(tag)) for token, tag in pos_tags]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Funktion zur Vorverarbeitung von Dokumenten für den NLTK Lemmatizer\n",
    "def preprocess_documents_nltk(documents):\n",
    "    \"\"\"Anwendet Lemmatization auf alle Dokumente.\"\"\"\n",
    "    for doc in documents:\n",
    "        doc['text'] = lemmatize_text_nltk(doc['text'])\n",
    "        yield doc\n",
    "\n",
    "# Definition der Funktion zum Erstellen eines Index für NLTK Lemmatizer\n",
    "def create_index_nltk(documents, stopwords):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords)\n",
    "    index_ref = indexer.index(preprocess_documents_nltk(documents))\n",
    "    return pt.IndexFactory.of(index_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition der Funktion zur Lemmatization eines Textes mit SpaCy\n",
    "def lemmatize_text_spacy(text):\n",
    "    \"\"\"Lemmatiziert den gegebenen Text mit SpaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Definition der Funktion zur Vorverarbeitung von Dokumenten\n",
    "def preprocess_documents_spacy(documents):\n",
    "    \"\"\"Anwendet Lemmatization auf alle Dokumente.\"\"\"\n",
    "    for doc in documents:\n",
    "        doc['text'] = lemmatize_text_spacy(doc['text'])\n",
    "        yield doc\n",
    "\n",
    "# Definition der Funktion zum Erstellen eines Index\n",
    "def create_index_spacy(documents, stopwords):\n",
    "    indexer = pt.IterDictIndexer(\"/tmp/index\", overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords)\n",
    "    index_ref = indexer.index(preprocess_documents_spacy(documents))\n",
    "    return pt.IndexFactory.of(index_ref)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funktion um eigene Indecies zu erstellen\n",
    "def create_index(path, documents, stopwords, stemmer):\n",
    "    indexer = pt.IterDictIndexer(path, overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords, stemmer=stemmer)\n",
    "    index_ref = indexer.index(documents)\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Funktion um aus einem txt-file eine Python Liste zu machen\n",
    "def read_text_file_to_array(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            # Remove newline characters and convert to integers\n",
    "            array = [(line.strip()) for line in lines]\n",
    "            return array\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die mittels fine tuning verbesserte Stopwordliste auf Basis von der Pyterrier Standard Stopwordliste\n",
    "terrier_costum_stopwords = read_text_file_to_array('../terrier-costum.txt')\n",
    "#Stopwordliste, aus der Pyterrier Stopwordliste, aber nur mit den Worten die in den Querys vorkommen auf die getestet wird\n",
    "terrier_query_occur_stopwords = read_text_file_to_array('../terrier-query.txt')\n",
    "#Stopwordliste, welche von ChatGPT kommt\n",
    "chatgpt_stopwords = read_text_file_to_array('../chatgpt-stopwordlist.txt')\n",
    "\n",
    "\n",
    "#Standardindex von Tira, welcher die ganzen Standardsettings von Pyterrier benutzt\n",
    "standard_index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "\n",
    "#Index der optimierten Pyterrier Stopwordliste\n",
    "terrier_costum_stopword_index = create_index('/tmp/index/', pt_dataset.get_corpus_iter(), terrier_costum_stopwords, 'porter')\n",
    "\n",
    "#Index zu der query Vorkommen Stopwordliste\n",
    "terrier_query_occur_stopword_index = create_index('/tmp/index1/', pt_dataset.get_corpus_iter(), terrier_query_occur_stopwords, 'porter')\n",
    "\n",
    "#Index zur ChatGPT Stopwordlist\n",
    "chatgpt_stopword_index = create_index('/tmp/index2/', pt_dataset.get_corpus_iter(), chatgpt_stopwords, 'porter')\n",
    "\n",
    "#Index ohne Stopwords\n",
    "nostopwords_index = create_index('/tmp/index3/', pt_dataset.get_corpus_iter(), [], 'porter')\n",
    "\n",
    "\n",
    "#Die jeweiligen Retrievalmodelle mit Weightingmodell XSqrA_M\n",
    "XSqrA_M = pt.BatchRetrieve(standard_index, wmodel='XSqrA_M')\n",
    "XSqrA_M_terrier_costum_stopwords = pt.BatchRetrieve(terrier_costum_stopword_index, wmodel='XSqrA_M')\n",
    "XSqrA_M_terrier_query_occur_stopwords = pt.BatchRetrieve(terrier_query_occur_stopword_index, wmodel='XSqrA_M')\n",
    "XSqrA_M_chatgpt_stopwords = pt.BatchRetrieve(chatgpt_stopword_index, wmodel='XSqrA_M')\n",
    "XSqrA_M_nostopwords = pt.BatchRetrieve(nostopwords_index, wmodel='XSqrA_M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bei STopwords die Stopwords eingeben die man will\n",
    "costum_index_nltk =create_index_nltk(pt_dataset.get_corpus_iter(), stopwords)\n",
    "costum_index_spacy = create_index_spacy(pt_dataset.get_corpus_iter(), stopwords)\n",
    "\n",
    "# Lade Index von tira\n",
    "index = tira.pt.index('ir-lab-sose-2024/tira-ir-starter/Index (tira-ir-starter-pyterrier)', pt_dataset)\n",
    "\n",
    "# Erstellen der BatchRetrieve-Instanzen\n",
    "XSqrA_M = pt.BatchRetrieve(index, wmodel=\"XSqrA_M\")\n",
    "XSqrA_M_nltk = pt.BatchRetrieve(costum_index_nltk, wmodel=\"XSqrA_M\")\n",
    "XSqrA_M_spacy = pt.BatchRetrieve(costum_index_spacy, wmodel=\"XSqrA_M\")\n",
    "\n",
    "# Experiment wie unten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are multiple query fields available: ('text', 'title', 'query', 'description', 'narrative'). To use with pyterrier, provide variant or modify dataframe to add query column.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>ndcg_cut.10</th>\n",
       "      <th>recip_rank</th>\n",
       "      <th>recall_100</th>\n",
       "      <th>P_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XSqrA_M</td>\n",
       "      <td>0.448211</td>\n",
       "      <td>0.650289</td>\n",
       "      <td>0.592214</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XSqrA_M_terrier_costum_stopwords</td>\n",
       "      <td>0.455100</td>\n",
       "      <td>0.664894</td>\n",
       "      <td>0.593577</td>\n",
       "      <td>0.407353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XSqrA_M_terrier_query_occur_stopwords</td>\n",
       "      <td>0.451725</td>\n",
       "      <td>0.661387</td>\n",
       "      <td>0.591281</td>\n",
       "      <td>0.401471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XSqrA_M_chatgpt_stopwords</td>\n",
       "      <td>0.433872</td>\n",
       "      <td>0.656496</td>\n",
       "      <td>0.593599</td>\n",
       "      <td>0.379412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XSqrA_M_nostopwords</td>\n",
       "      <td>0.424464</td>\n",
       "      <td>0.666510</td>\n",
       "      <td>0.585498</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  ndcg_cut.10  recip_rank  recall_100  \\\n",
       "0                                XSqrA_M     0.448211    0.650289    0.592214   \n",
       "1       XSqrA_M_terrier_costum_stopwords     0.455100    0.664894    0.593577   \n",
       "2  XSqrA_M_terrier_query_occur_stopwords     0.451725    0.661387    0.591281   \n",
       "3              XSqrA_M_chatgpt_stopwords     0.433872    0.656496    0.593599   \n",
       "4                    XSqrA_M_nostopwords     0.424464    0.666510    0.585498   \n",
       "\n",
       "       P_10  \n",
       "0  0.400000  \n",
       "1  0.407353  \n",
       "2  0.401471  \n",
       "3  0.379412  \n",
       "4  0.375000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Evaluation\n",
    "pt.Experiment(\n",
    "    [\n",
    "        XSqrA_M,\n",
    "        XSqrA_M_terrier_costum_stopwords,\n",
    "        XSqrA_M_terrier_query_occur_stopwords,\n",
    "        XSqrA_M_chatgpt_stopwords,\n",
    "        XSqrA_M_nostopwords\n",
    "    ],\n",
    "    pt_dataset.get_topics(),\n",
    "    pt_dataset.get_qrels(),\n",
    "    [\"ndcg_cut.10\", \"recip_rank\", \"recall_100\", \"P_10\"],\n",
    "    names = [\n",
    "        \"XSqrA_M\",\n",
    "        \"XSqrA_M_terrier_costum_stopwords\",\n",
    "        \"XSqrA_M_terrier_query_occur_stopwords\",\n",
    "        \"XSqrA_M_chatgpt_stopwords\",\n",
    "        \"XSqrA_M_nostopwords\"\n",
    "    ]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
