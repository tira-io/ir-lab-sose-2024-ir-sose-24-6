{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IR Lab SoSe 2024 Team 6: Grid Search\n",
    "\n",
    "This jupyter notebook is a grid search over all the customizations we looked at regarding Stopwords, Stemming and Lemmatization, and different retrieval systems.\n",
    "For each of these topics we individually tested a few of the typical types and test the best two of each in this grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tira in /usr/local/lib/python3.10/dist-packages (0.0.132)\n",
      "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.10/dist-packages (0.5.5)\n",
      "Requirement already satisfied: python-terrier in /usr/local/lib/python3.10/dist-packages (0.10.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
      "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tira) (4.66.1)\n",
      "Requirement already satisfied: requests==2.*,>=2.26 in /usr/local/lib/python3.10/dist-packages (from tira) (2.31.0)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tira) (2.1.3)\n",
      "Requirement already satisfied: docker==6.*,>=6.0.0 in /usr/local/lib/python3.10/dist-packages (from tira) (6.1.3)\n",
      "Requirement already satisfied: packaging>=14.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (23.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (1.7.0)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker==6.*,>=6.0.0->tira) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (2023.11.17)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.*,>=2.26->tira) (3.3.2)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.12.2)\n",
      "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.6)\n",
      "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.3.2)\n",
      "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (4.9.3)\n",
      "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.3)\n",
      "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (1.26.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (6.0.1)\n",
      "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (2.3.2)\n",
      "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.5)\n",
      "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.6)\n",
      "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.2.2)\n",
      "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (3.2.3)\n",
      "Requirement already satisfied: pyautocorpus>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from ir-datasets) (0.1.12)\n",
      "Requirement already satisfied: matchpy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.5)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from python-terrier) (10.1.0)\n",
      "Requirement already satisfied: ir-measures>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.1.2)\n",
      "Requirement already satisfied: nptyping==1.4.4 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.4.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.11.4)\n",
      "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (from python-terrier) (3.2)\n",
      "Requirement already satisfied: pyjnius>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.6.1)\n",
      "Requirement already satisfied: chest in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.2.3)\n",
      "Requirement already satisfied: statsmodels in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.14.0)\n",
      "Requirement already satisfied: deprecated in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.2.14)\n",
      "Requirement already satisfied: pytrec-eval-terrier>=0.5.3 in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.5.6)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from python-terrier) (1.3.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from python-terrier) (0.3.7)\n",
      "Requirement already satisfied: typish>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from nptyping==1.4.4->python-terrier) (1.9.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.4)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.5)\n",
      "Requirement already satisfied: cwl-eval>=1.0.10 in /usr/local/lib/python3.10/dist-packages (from ir-measures>=0.3.1->python-terrier) (1.0.12)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.8.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
      "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
      "Requirement already satisfied: heapdict in /usr/local/lib/python3.10/dist-packages (from chest->python-terrier) (1.0.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->python-terrier) (2.1.3)\n",
      "Requirement already satisfied: multiset<3.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from matchpy->python-terrier) (2.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->tira) (2.8.2)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from statsmodels->python-terrier) (0.5.4)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.2->statsmodels->python-terrier) (1.16.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "#Standard Imports für alle Notebooks\n",
    "\n",
    "!pip3 install tira ir-datasets python-terrier nltk scikit-learn spacy\n",
    "\n",
    "from tira.third_party_integrations import ensure_pyterrier_is_loaded, persist_and_normalize_run\n",
    "from tira.rest_api_client import Client\n",
    "import pyterrier as pt\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ensure_pyterrier_is_loaded()\n",
    "tira = Client()\n",
    "\n",
    "pt_dataset = pt.get_dataset('irds:ir-lab-sose-2024/ir-acl-anthology-20240504-training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (59.6.0)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.12.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.4)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.8.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2023.11.17)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.7.1)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.18.1)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.17.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Laden der NLTK Ressourcen\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Laden der SpaCy-Ressourcen\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Laden des SpaCy-Modells\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode um Beschreibung des POS-Tags zu bekommen für den NLTK Lemmatizer\n",
    "def get_wordnet_pos_nltk(treebank_tag):\n",
    "    \"\"\"Konvertiert POS-Tag in ein Format, das vom WordNet-Lemmatizer unterstützt wird.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# Funktion um den Text zu lemmatizen für NLTK Lemmatizer\n",
    "def lemmatize_text_nltk(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos_nltk(tag)) for token, tag in pos_tags]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Definition der Funktion zur Lemmatization eines Textes mit SpaCy\n",
    "def lemmatize_text_spacy(text):\n",
    "    \"\"\"Lemmatiziert den gegebenen Text mit SpaCy.\"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "def preprocess_documents(documents, method):\n",
    "    if method == 'nltk':\n",
    "        lemmatize_text = lemmatize_text_nltk\n",
    "    elif method == 'spacy':\n",
    "        lemmatize_text = lemmatize_text_spacy\n",
    "    else:\n",
    "        raise ValueError(\"Invalid method specified. Use 'nltk' or 'spacy'.\")\n",
    "\n",
    "    for doc in documents:\n",
    "        doc['text'] = lemmatize_text(doc['text'])\n",
    "        yield doc\n",
    "\n",
    "#Funktion um eigene Indecies zu erstellen\n",
    "def create_index(path, documents, stopwords, stemmer):\n",
    "    indexer = pt.IterDictIndexer(path, overwrite=True, meta={'docno': 100, 'text': 20480}, stopwords=stopwords, stemmer=stemmer)\n",
    "    index_ref = indexer.index(documents)\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "\n",
    "#Funktion um aus einem txt-file eine Python Liste zu machen\n",
    "def read_text_file_to_array(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            # Remove newline characters and convert to integers\n",
    "            array = [(line.strip()) for line in lines]\n",
    "            return array\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} not found.\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ir-lab-sose-2024/ir-acl-anthology-20240504-training documents:   0%|          | 0/126958 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13:16:30.168 [ForkJoinPool-9-worker-3] WARN org.terrier.structures.indexing.Indexer - TermPipeline object org.terrier.terms.LemurKrovetzStemmer not found: java.lang.ClassNotFoundException: org.terrier.terms.LemurKrovetzStemmer\n",
      "13:16:30.205 [ForkJoinPool-9-worker-3] ERROR org.terrier.structures.Index - Cannot create new index: path /workspaces/ir-lab-sose-2024-ir-sose-24-6/gridsearch/./var/tmp/index1/ does not exist, or cannot be written to\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "java.lang.ClassNotFoundException: org.terrier.terms.LemurKrovetzStemmer\n",
      "\tat java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:581)\n",
      "\tat java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:178)\n",
      "\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n",
      "\tat java.base/java.lang.Class.forName0(Native Method)\n",
      "\tat java.base/java.lang.Class.forName(Class.java:398)\n",
      "\tat org.terrier.utility.ApplicationSetup.getClass(ApplicationSetup.java:416)\n",
      "\tat org.terrier.structures.indexing.Indexer.load_pipeline(Indexer.java:323)\n",
      "\tat org.terrier.structures.indexing.Indexer.init(Indexer.java:197)\n",
      "\tat org.terrier.structures.indexing.classical.BasicIndexer.<init>(BasicIndexer.java:183)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\n",
      "\tat org.terrier.python.ParallelIndexer$3.apply(ParallelIndexer.java:126)\n",
      "\tat org.terrier.python.ParallelIndexer$3.apply(ParallelIndexer.java:120)\n",
      "\tat java.base/java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:195)\n",
      "\tat java.base/java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:484)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:474)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:952)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceTask.doLeaf(ReduceOps.java:926)\n",
      "\tat java.base/java.util.stream.AbstractTask.compute(AbstractTask.java:327)\n",
      "\tat java.base/java.util.concurrent.CountedCompleter.exec(CountedCompleter.java:746)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doInvoke(ForkJoinTask.java:408)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.invoke(ForkJoinTask.java:736)\n",
      "\tat java.base/java.util.stream.ReduceOps$ReduceOp.evaluateParallel(ReduceOps.java:919)\n",
      "\tat java.base/java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:233)\n",
      "\tat java.base/java.util.stream.ReferencePipeline.reduce(ReferencePipeline.java:558)\n",
      "\tat org.terrier.python.ParallelIndexer$4.call(ParallelIndexer.java:140)\n",
      "\tat org.terrier.python.ParallelIndexer$4.call(ParallelIndexer.java:137)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$AdaptedCallable.exec(ForkJoinTask.java:1448)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:290)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1020)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1656)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1594)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:183)\n"
     ]
    },
    {
     "ename": "JavaException",
     "evalue": "JVM exception occurred: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Cannot create new index: path /workspaces/ir-lab-sose-2024-ir-sose-24-6/gridsearch/./var/tmp/index1/ does not exist, or cannot be written to java.lang.RuntimeException",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJavaException\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 7\u001b[0m\n\u001b[1;32m      3\u001b[0m chatgpt_stopwords \u001b[38;5;241m=\u001b[39m read_text_file_to_array(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../chatgpt-stopwordlist.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Naming: [Stemmer/Lemmatizer]_[Type]__[StopwordList]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m indices \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_L_T\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mcreate_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtmp/index1/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_corpus_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mterrier_custom_stopwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLemurKrovetzStemmer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_L_C\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index2/\u001b[39m\u001b[38;5;124m'\u001b[39m, pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), chatgpt_stopwords, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemurKrovetzStemmer\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_L_N\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index3/\u001b[39m\u001b[38;5;124m'\u001b[39m, pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLemurKrovetzStemmer\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_P_T\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index4/\u001b[39m\u001b[38;5;124m'\u001b[39m, pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), terrier_custom_stopwords, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRv2PorterStemmer\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_P_C\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index5/\u001b[39m\u001b[38;5;124m'\u001b[39m, pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), chatgpt_stopwords, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRv2PorterStemmer\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_P_N\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index6/\u001b[39m\u001b[38;5;124m'\u001b[39m, pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), [], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTRv2PorterStemmer\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_N_T\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index7/\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocess_documents(pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), nltk), terrier_custom_stopwords, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_N_C\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index8/\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocess_documents(pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), nltk), chatgpt_stopwords, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_N_N\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index9/\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocess_documents(pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), nltk), [], \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_S_T\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index10/\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocess_documents(pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), spacy), terrier_custom_stopwords, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_S_C\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index11/\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocess_documents(pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), spacy), chatgpt_stopwords, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_S_N\u001b[39m\u001b[38;5;124m\"\u001b[39m: create_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmp/index12/\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocess_documents(pt_dataset\u001b[38;5;241m.\u001b[39mget_corpus_iter(), spacy), [], \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m }\n\u001b[1;32m     21\u001b[0m retrieval_models \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_L_T_XSqrA_M\u001b[39m\u001b[38;5;124m\"\u001b[39m: pt\u001b[38;5;241m.\u001b[39mBatchRetrieve(indices[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_L_T\u001b[39m\u001b[38;5;124m\"\u001b[39m], wmodel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mXSqrA_M\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_L_T_BM25\u001b[39m\u001b[38;5;124m\"\u001b[39m: pt\u001b[38;5;241m.\u001b[39mBatchRetrieve(indices[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mS_L_T\u001b[39m\u001b[38;5;124m\"\u001b[39m], wmodel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBM25\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_S_N_DPH\u001b[39m\u001b[38;5;124m\"\u001b[39m: pt\u001b[38;5;241m.\u001b[39mBatchRetrieve(indices[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL_S_N\u001b[39m\u001b[38;5;124m\"\u001b[39m], wmodel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDPH\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     69\u001b[0m }\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Evaluation\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 45\u001b[0m, in \u001b[0;36mcreate_index\u001b[0;34m(path, documents, stopwords, stemmer)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_index\u001b[39m(path, documents, stopwords, stemmer):\n\u001b[1;32m     44\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m pt\u001b[38;5;241m.\u001b[39mIterDictIndexer(path, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, meta\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdocno\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m20480\u001b[39m}, stopwords\u001b[38;5;241m=\u001b[39mstopwords, stemmer\u001b[38;5;241m=\u001b[39mstemmer)\n\u001b[0;32m---> 45\u001b[0m     index_ref \u001b[38;5;241m=\u001b[39m \u001b[43mindexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pt\u001b[38;5;241m.\u001b[39mIndexFactory\u001b[38;5;241m.\u001b[39mof(index_ref)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyterrier/index.py:1055\u001b[0m, in \u001b[0;36m_IterDictIndexer_fifo.index\u001b[0;34m(self, it, fields, meta, meta_lengths)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         ParallelIndexer\u001b[38;5;241m.\u001b[39mbuildParallelTokenised(j_collections, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_dir, Indexer, Merger)\n\u001b[1;32m   1054\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1055\u001b[0m         \u001b[43mParallelIndexer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuildParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mj_collections\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIndexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMerger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m indexref \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m IndexFactory\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:876\u001b[0m, in \u001b[0;36mjnius.JavaMethod.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_export_class.pxi:1059\u001b[0m, in \u001b[0;36mjnius.JavaMethod.call_staticmethod\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mjnius/jnius_utils.pxi:79\u001b[0m, in \u001b[0;36mjnius.check_exception\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mJavaException\u001b[0m: JVM exception occurred: java.util.concurrent.ExecutionException: java.lang.IllegalArgumentException: java.lang.IllegalArgumentException: Cannot create new index: path /workspaces/ir-lab-sose-2024-ir-sose-24-6/gridsearch/./var/tmp/index1/ does not exist, or cannot be written to java.lang.RuntimeException"
     ]
    }
   ],
   "source": [
    "# Load custom stopword lists\n",
    "terrier_custom_stopwords = read_text_file_to_array('../terrier-custom.txt')\n",
    "chatgpt_stopwords = read_text_file_to_array('../chatgpt-stopwordlist.txt')\n",
    "\n",
    "# Naming: [Stemmer/Lemmatizer]_[Type]__[StopwordList]\n",
    "indices = {\n",
    "    \"S_L_T\": create_index('tmp/index1/', pt_dataset.get_corpus_iter(), terrier_custom_stopwords, 'LemurKrovetzStemmer'),\n",
    "    \"S_L_C\": create_index('tmp/index2/', pt_dataset.get_corpus_iter(), chatgpt_stopwords, 'LemurKrovetzStemmer'),\n",
    "    \"S_L_N\": create_index('tmp/index3/', pt_dataset.get_corpus_iter(), [], 'LemurKrovetzStemmer'),\n",
    "    \"S_P_T\": create_index('tmp/index4/', pt_dataset.get_corpus_iter(), terrier_custom_stopwords, 'TRv2PorterStemmer'),\n",
    "    \"S_P_C\": create_index('tmp/index5/', pt_dataset.get_corpus_iter(), chatgpt_stopwords, 'TRv2PorterStemmer'),\n",
    "    \"S_P_N\": create_index('tmp/index6/', pt_dataset.get_corpus_iter(), [], 'TRv2PorterStemmer'),\n",
    "    \"L_N_T\": create_index('tmp/index7/', preprocess_documents(pt_dataset.get_corpus_iter(), nltk), terrier_custom_stopwords, None),\n",
    "    \"L_N_C\": create_index('tmp/index8/', preprocess_documents(pt_dataset.get_corpus_iter(), nltk), chatgpt_stopwords, None),\n",
    "    \"L_N_N\": create_index('tmp/index9/', preprocess_documents(pt_dataset.get_corpus_iter(), nltk), [], None),\n",
    "    \"L_S_T\": create_index('tmp/index10/', preprocess_documents(pt_dataset.get_corpus_iter(), spacy), terrier_custom_stopwords, None),\n",
    "    \"L_S_C\": create_index('tmp/index11/', preprocess_documents(pt_dataset.get_corpus_iter(), spacy), chatgpt_stopwords, None),\n",
    "    \"L_S_N\": create_index('tmp/index12/', preprocess_documents(pt_dataset.get_corpus_iter(), spacy), [], None)\n",
    "}\n",
    "\n",
    "retrieval_models = {\n",
    "    \"S_L_T_XSqrA_M\": pt.BatchRetrieve(indices[\"S_L_T\"], wmodel=\"XSqrA_M\"),\n",
    "    \"S_L_T_BM25\": pt.BatchRetrieve(indices[\"S_L_T\"], wmodel=\"BM25\"),\n",
    "    \"S_L_T_DPH\": pt.BatchRetrieve(indices[\"S_L_T\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"S_L_C_XSqrA_M\": pt.BatchRetrieve(indices[\"S_L_C\"], wmodel=\"XSqrA_M\"),\n",
    "    \"S_L_C_BM25\": pt.BatchRetrieve(indices[\"S_L_C\"], wmodel=\"BM25\"),\n",
    "    \"S_L_C_DPH\": pt.BatchRetrieve(indices[\"S_L_C\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"S_L_N_XSqrA_M\": pt.BatchRetrieve(indices[\"S_L_N\"], wmodel=\"XSqrA_M\"),\n",
    "    \"S_L_N_BM25\": pt.BatchRetrieve(indices[\"S_L_N\"], wmodel=\"BM25\"),\n",
    "    \"S_L_N_DPH\": pt.BatchRetrieve(indices[\"S_L_N\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"S_P_T_XSqrA_M\": pt.BatchRetrieve(indices[\"S_P_T\"], wmodel=\"XSqrA_M\"),\n",
    "    \"S_P_T_BM25\": pt.BatchRetrieve(indices[\"S_P_T\"], wmodel=\"BM25\"),\n",
    "    \"S_P_T_DPH\": pt.BatchRetrieve(indices[\"S_P_T\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"S_P_C_XSqrA_M\": pt.BatchRetrieve(indices[\"S_P_C\"], wmodel=\"XSqrA_M\"),\n",
    "    \"S_P_C_BM25\": pt.BatchRetrieve(indices[\"S_P_C\"], wmodel=\"BM25\"),\n",
    "    \"S_P_C_DPH\": pt.BatchRetrieve(indices[\"S_P_C\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"S_P_N_XSqrA_M\": pt.BatchRetrieve(indices[\"S_P_N\"], wmodel=\"XSqrA_M\"),\n",
    "    \"S_P_N_BM25\": pt.BatchRetrieve(indices[\"S_P_N\"], wmodel=\"BM25\"),\n",
    "    \"S_P_N_DPH\": pt.BatchRetrieve(indices[\"S_P_N\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"L_N_T_XSqrA_M\": pt.BatchRetrieve(indices[\"L_N_T\"], wmodel=\"XSqrA_M\"),\n",
    "    \"L_N_T_BM25\": pt.BatchRetrieve(indices[\"L_N_T\"], wmodel=\"BM25\"),\n",
    "    \"L_N_T_DPH\": pt.BatchRetrieve(indices[\"L_N_T\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"L_N_C_XSqrA_M\": pt.BatchRetrieve(indices[\"L_N_C\"], wmodel=\"XSqrA_M\"),\n",
    "    \"L_N_C_BM25\": pt.BatchRetrieve(indices[\"L_N_C\"], wmodel=\"BM25\"),\n",
    "    \"L_N_C_DPH\": pt.BatchRetrieve(indices[\"L_N_C\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"L_N_N_XSqrA_M\": pt.BatchRetrieve(indices[\"L_N_N\"], wmodel=\"XSqrA_M\"),\n",
    "    \"L_N_N_BM25\": pt.BatchRetrieve(indices[\"L_N_N\"], wmodel=\"BM25\"),\n",
    "    \"L_N_N_DPH\": pt.BatchRetrieve(indices[\"L_N_N\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"L_S_T_XSqrA_M\": pt.BatchRetrieve(indices[\"L_S_T\"], wmodel=\"XSqrA_M\"),\n",
    "    \"L_S_T_BM25\": pt.BatchRetrieve(indices[\"L_S_T\"], wmodel=\"BM25\"),\n",
    "    \"L_S_T_DPH\": pt.BatchRetrieve(indices[\"L_S_T\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"L_S_C_XSqrA_M\": pt.BatchRetrieve(indices[\"L_S_C\"], wmodel=\"XSqrA_M\"),\n",
    "    \"L_S_C_BM25\": pt.BatchRetrieve(indices[\"L_S_C\"], wmodel=\"BM25\"),\n",
    "    \"L_S_C_DPH\": pt.BatchRetrieve(indices[\"L_S_C\"], wmodel=\"DPH\"),\n",
    "    \n",
    "    \"L_S_N_XSqrA_M\": pt.BatchRetrieve(indices[\"L_S_N\"], wmodel=\"XSqrA_M\"),\n",
    "    \"L_S_N_BM25\": pt.BatchRetrieve(indices[\"L_S_N\"], wmodel=\"BM25\"),\n",
    "    \"L_S_N_DPH\": pt.BatchRetrieve(indices[\"L_S_N\"], wmodel=\"DPH\"),\n",
    "}\n",
    "\n",
    "# Evaluation\n",
    "pt.Experiment(\n",
    "    list(retrieval_models.values()),\n",
    "    pt_dataset.get_topics(),\n",
    "    pt_dataset.get_qrels(),\n",
    "    [\"ndcg_cut.10\", \"recip_rank\", \"recall_100\", \"P_10\"],\n",
    "    names=list(retrieval_models.keys())\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
